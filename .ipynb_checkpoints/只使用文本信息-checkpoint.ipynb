{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dutir/chenxiaoyu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(100)\n",
    "\n",
    "import os\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "def init_env(gpu_id='0'):\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES']=gpu_id\n",
    "    \n",
    "    config=tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    set_session(tf.Session(config=config))\n",
    "init_env()\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from database import SmpCupText\n",
    "db=SmpCupText('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------- data loaded ---------------------------------------\n",
    "#-----------------------------------------------------------------\n",
    "from keras.layers import Input,Dropout,Dense,LSTM,merge,MaxPooling1D,Conv1D\n",
    "from keras_extension.layers import AttentionPoolingLayer,Embedding,ConcatLayer,ConvDense\n",
    "from keras_extension.models import YModel\n",
    "from keras_extension.utils import pad_tensor3d\n",
    "import jieba\n",
    "import re\n",
    "class AttentionNetwork(object):#AttentionNetwork(a),output_dim=a\n",
    "    def __init__(self,output_dim=8):\n",
    "        self.output_dim=output_dim\n",
    "        self.build()\n",
    "        \n",
    "        pass\n",
    "    def build(self):\n",
    "        input=Input(shape=(None,None),dtype='int32')\n",
    "        \n",
    "        #------------ word vectors pooling to weibo vector ------------\n",
    "        embedding_layer=Embedding(input_dim=db.weights.shape[0],output_dim=300,weights=[db.weights],trainable=True,mask_zero=True)\n",
    "        # 32x200x113x300\n",
    "        x=embedding_layer(input)\n",
    "        x=Dropout(0.5)(x)\n",
    "        \n",
    "        \n",
    "        word_att_layer=AttentionPoolingLayer(128,)\n",
    "        x_weibo=word_att_layer(x)\n",
    "        #------------ weibo vectors pooling to user vector -----------\n",
    "        x=ConvDense(128,activation='relu')(x_weibo)\n",
    "        x=Dropout(0.5)(x)\n",
    "\n",
    "        x=AttentionPoolingLayer(128)(x)\n",
    "        x=Dropout(0.5)(x)\n",
    "      \n",
    "        x=Dense(100,activation='tanh')(x)\n",
    "        \n",
    "        x=Dropout(0.5)(x)\n",
    "        output=Dense(self.output_dim,activation='softmax')(x)\n",
    "        model=YModel(input=input,output=output)\n",
    "        model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "        self.model=model\n",
    "    \n",
    "    def fit(self,train_data,test_data,filename='best.model',adaboost=False):\n",
    "        self.model.fit_on_batch(train_data,test_data,n_earlystop=20,\n",
    "                                cnt_in_epoch=100,filename=filename,\n",
    "                                n_epoch=50,adaboost=adaboost,best_type='best_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dutir/chenxiaoyu/bishe_to_chen/keras_extension/models.py:6: UserWarning: Update your `YModel` call to the Keras 2 API: `YModel(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  super(YModel, self).__init__(**kwargs)\n",
      "/home/dutir/chenxiaoyu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 110800200 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 train loss 0.5445204317569733 acc 0.76     \n",
      "testing...best_acc\n",
      " 1 test 16:30:40 loss:0.469098, acc:0.780851\n",
      "-----\n",
      " 2 train loss 0.5192165881694861 acc 0.7726130653266332     \n",
      "testing...best_acc\n",
      " 2 test 16:30:48 loss:0.428470, acc:0.804255\n",
      "-----\n",
      " 3 train loss 0.5040917856871483 acc 0.785175879396985     \n",
      " 3 test 16:30:53 loss:0.473256, acc:0.791489\n",
      "-----\n",
      " 4 train loss 0.4267037590127438 acc 0.81     \n",
      "testing...best_acc\n",
      " 4 test 16:31:01 loss:0.380618, acc:0.839362\n",
      "-----\n",
      " 5 train loss 0.4687603897973895 acc 0.79375     \n",
      " 5 test 16:31:07 loss:0.396368, acc:0.822340\n",
      "-----\n",
      " 6 train loss 0.39150628764182327 acc 0.83     \n",
      " 6 test 16:31:12 loss:0.390516, acc:0.830851\n",
      "-----\n",
      " 7 train loss 0.4240902923233807 acc 0.81375     \n",
      "testing...best_acc\n",
      " 7 test 16:31:20 loss:0.368448, acc:0.841489\n",
      "-----\n",
      " 8 train loss 0.4235181239619851 acc 0.81875     \n",
      " 8 test 16:31:26 loss:0.405924, acc:0.829787\n",
      "-----\n",
      " 9 train loss 0.4233793256431818 acc 0.82375     \n",
      " 9 test 16:31:31 loss:0.383549, acc:0.825532\n",
      "-----\n",
      " 10 train loss 0.42615624392405155 acc 0.805     \n",
      " 10 test 16:31:37 loss:0.382594, acc:0.836170\n",
      "-----\n",
      " 11 train loss 0.3988200989551842 acc 0.835     \n",
      " 11 test 16:31:42 loss:0.399615, acc:0.811702\n",
      "-----\n",
      " 12 train loss 0.42914202362298964 acc 0.81625     \n",
      " 12 test 16:31:47 loss:0.527689, acc:0.729787\n",
      "-----\n",
      " 13 train loss 0.3998325660824776 acc 0.83375     \n",
      "testing...best_acc\n",
      " 13 test 16:31:56 loss:0.377583, acc:0.860638\n",
      "-----\n",
      " 14 train loss 0.3745546232536435 acc 0.84125     \n",
      " 14 test 16:32:01 loss:0.357251, acc:0.847872\n",
      "-----\n",
      " 15 train loss 0.3330207414738834 acc 0.86625     \n",
      " 15 test 16:32:06 loss:0.362331, acc:0.855319\n",
      "-----\n",
      " 16 train loss 0.3755633282288909 acc 0.845     \n",
      " 16 test 16:32:12 loss:0.342183, acc:0.857447\n",
      "-----\n",
      " 17 train loss 0.36437252536416054 acc 0.84875     \n",
      " 17 test 16:32:17 loss:0.363826, acc:0.834043\n",
      "-----\n",
      " 18 train loss 0.3412708346918225 acc 0.86125     \n",
      " 18 test 16:32:22 loss:0.349301, acc:0.840426\n",
      "-----\n",
      " 19 train loss 0.38129013538360595 acc 0.83125     \n",
      " 19 test 16:32:28 loss:0.355968, acc:0.851064\n",
      "-----\n",
      " 20 train loss 0.39252347592264414 acc 0.835     \n",
      " 20 test 16:32:33 loss:0.365600, acc:0.847872\n",
      "-----\n",
      " 21 train loss 0.36506542805582287 acc 0.83375     \n",
      "testing...best_acc\n",
      " 21 test 16:32:42 loss:0.333830, acc:0.871277\n",
      "-----\n",
      " 22 train loss 0.32624945376068354 acc 0.85875     \n",
      " 22 test 16:32:47 loss:0.371184, acc:0.834043\n",
      "-----\n",
      " 23 train loss 0.3647906618565321 acc 0.8525     \n",
      " 23 test 16:32:52 loss:0.382104, acc:0.862766\n",
      "-----\n",
      " 24 train loss 0.35379061005522855 acc 0.8479899497487438     \n",
      " 24 test 16:32:58 loss:0.355211, acc:0.855319\n",
      "-----\n",
      " 25 train loss 0.3349570693261921 acc 0.87375     \n",
      " 25 test 16:33:03 loss:0.357191, acc:0.858511\n",
      "-----\n",
      " 26 train loss 0.32934996601194144 acc 0.86875     \n",
      " 26 test 16:33:09 loss:0.381207, acc:0.824468\n",
      "-----\n",
      " 27 train loss 0.3238075191527605 acc 0.86875     \n",
      " 27 test 16:33:14 loss:0.396572, acc:0.852128\n",
      "-----\n",
      " 28 train loss 0.34681809480166315 acc 0.8592964824120602     \n",
      " 28 test 16:33:20 loss:0.348370, acc:0.855319\n",
      "-----\n",
      " 29 train loss 0.29781623583287 acc 0.88375     \n",
      " 29 test 16:33:25 loss:0.379363, acc:0.850000\n",
      "-----\n",
      " 30 train loss 0.307758434638381 acc 0.87625     \n",
      " 30 test 16:33:30 loss:0.355590, acc:0.856383\n",
      "-----\n",
      " 31 train loss 0.28138303177431223 acc 0.89     \n",
      " 31 test 16:33:36 loss:0.354334, acc:0.853191\n",
      "-----\n",
      " 32 train loss 0.2581745501251976 acc 0.8969849246231156     \n",
      " 32 test 16:33:41 loss:0.383401, acc:0.862766\n",
      "-----\n",
      " 33 train loss 0.2889628362416023 acc 0.8806532663316583     \n",
      " 33 test 16:33:46 loss:0.344878, acc:0.852128\n",
      "-----\n",
      " 34 train loss 0.2696772890165448 acc 0.90125     \n",
      " 34 test 16:33:52 loss:0.370892, acc:0.844681\n",
      "-----\n",
      " 35 train loss 0.26425143433734777 acc 0.905     \n",
      " 35 test 16:33:57 loss:0.346327, acc:0.848936\n",
      "-----\n",
      " 36 train loss 0.2245079807564616 acc 0.91625     \n",
      " 36 test 16:34:03 loss:0.351696, acc:0.854255\n",
      "-----\n",
      " 37 train loss 0.23551893171854318 acc 0.91     \n",
      " 37 test 16:34:08 loss:0.382309, acc:0.846809\n",
      "-----\n",
      " 38 train loss 0.20397558396682144 acc 0.9175     \n",
      " 38 test 16:34:14 loss:0.378658, acc:0.843617\n",
      "-----\n",
      " 39 train loss 0.16334723215317354 acc 0.9375     \n",
      " 39 test 16:34:19 loss:0.407461, acc:0.832979\n",
      "-----\n",
      " 40 train loss 0.13331968491955196 acc 0.94875     \n",
      " 40 test 16:34:25 loss:0.488039, acc:0.851064\n",
      "-----\n",
      " 41 train loss 0.09560767328483638 acc 0.9723618090452262     \n",
      " 41 test 16:34:30 loss:0.540020, acc:0.771277\n",
      "-----\n",
      " 42 train loss 0.06833536488335798 acc 0.9773869346733668     \n",
      " 42 test 16:34:36 loss:0.520007, acc:0.809574\n",
      "-----\n",
      "early stop\n",
      "best: 20 0.33383048332117976 0.8712765957446809\n"
     ]
    }
   ],
   "source": [
    "train_data,test_data=db.get_data(task=0,batch_size=8)\n",
    "gender_model=AttentionNetwork(2)\n",
    "gender_model.fit(train_data,test_data,'weibo_user_gender.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dutir/chenxiaoyu/bishe_to_chen/keras_extension/models.py:6: UserWarning: Update your `YModel` call to the Keras 2 API: `YModel(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  super(YModel, self).__init__(**kwargs)\n",
      "/home/dutir/chenxiaoyu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 110800200 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 train loss 0.9735392037034035 acc 0.5378125     \n",
      "testing...best_acc\n",
      " 1 test 20:58:52 loss:0.893544, acc:0.586290\n",
      "-----\n",
      " 2 train loss 0.9246699764579535 acc 0.571875     \n",
      "testing...best_acc\n",
      " 2 test 20:59:15 loss:0.886691, acc:0.612903\n",
      "-----\n",
      " 3 train loss 0.8946242979168892 acc 0.6053125     \n",
      "testing...best_acc\n",
      " 3 test 20:59:39 loss:0.855773, acc:0.625000\n",
      "-----\n",
      " 4 train loss 0.8631353896856307 acc 0.619375     \n",
      "testing...best_acc\n",
      " 4 test 21:00:03 loss:0.879369, acc:0.630645\n",
      "-----\n",
      " 5 train loss 0.837563077956438 acc 0.629375     \n",
      " 5 test 21:00:24 loss:0.870309, acc:0.616129\n",
      "-----\n",
      " 6 train loss 0.7858349802345037 acc 0.6515625     \n",
      " 6 test 21:00:45 loss:0.892633, acc:0.621774\n",
      "-----\n",
      " 7 train loss 0.5442685778066516 acc 0.78625     \n",
      " 7 test 21:01:06 loss:1.017316, acc:0.583065\n",
      "-----\n",
      " 8 train loss 0.27326980821409963 acc 0.9084375     \n",
      " 8 test 21:01:27 loss:1.175943, acc:0.588710\n",
      "-----\n",
      " 9 train loss 0.17704622982080764 acc 0.943125     \n",
      " 9 test 21:01:47 loss:1.309561, acc:0.591935\n",
      "-----\n",
      " 10 train loss 0.11421905657702154 acc 0.965     \n",
      " 10 test 21:02:08 loss:1.402253, acc:0.578226\n",
      "-----\n",
      " 11 train loss 0.06089957056462481 acc 0.9828125     \n",
      " 11 test 21:02:29 loss:1.529722, acc:0.574194\n",
      "-----\n",
      " 12 train loss 0.04133419272832697 acc 0.988125     \n",
      " 12 test 21:02:50 loss:2.044413, acc:0.576613\n",
      "-----\n",
      " 13 train loss 0.024041382383249824 acc 0.9921875     \n",
      " 13 test 21:03:10 loss:2.211600, acc:0.579839\n",
      "-----\n",
      " 14 train loss 0.012102804500223385 acc 0.996875     \n",
      " 14 test 21:03:31 loss:1.944100, acc:0.564516\n",
      "-----\n",
      " 15 train loss 0.014827756862277077 acc 0.9971875     \n",
      " 15 test 21:03:51 loss:2.221241, acc:0.550806\n",
      "-----\n",
      " 16 train loss 0.02113767427176537 acc 0.99625     \n",
      " 16 test 21:04:12 loss:2.232857, acc:0.568548\n",
      "-----\n",
      " 17 train loss 0.00276468441231426 acc 0.9990625     \n",
      " 17 test 21:04:32 loss:2.838986, acc:0.579032\n",
      "-----\n",
      " 18 train loss 0.004334793568777897 acc 0.99875     \n",
      " 18 test 21:04:53 loss:2.852929, acc:0.568548\n",
      "-----\n",
      " 19 train loss 0.005539256470294021 acc 0.999375     \n",
      " 19 test 21:05:14 loss:3.106112, acc:0.571774\n",
      "-----\n",
      " 20 train loss 0.00563617241481758 acc 0.99875     \n",
      " 20 test 21:05:34 loss:2.832360, acc:0.573387\n",
      "-----\n",
      " 21 train loss 0.0007062793409199486 acc 0.9996875     \n",
      " 21 test 21:05:55 loss:3.476156, acc:0.570968\n",
      "-----\n",
      " 22 train loss 0.004297290454994851 acc 0.999375     \n",
      " 22 test 21:06:16 loss:3.086364, acc:0.568548\n",
      "-----\n",
      " 23 train loss 0.008938050216539643 acc 0.9990625     \n",
      " 23 test 21:06:36 loss:4.004821, acc:0.579032\n",
      "-----\n",
      " 24 train loss 0.010673400788972102 acc 0.9990625     \n",
      " 24 test 21:06:57 loss:3.292492, acc:0.554839\n",
      "-----\n",
      " 25 train loss 0.0038075925626842987 acc 0.9984375     \n",
      " 25 test 21:07:18 loss:3.620911, acc:0.561290\n",
      "-----\n",
      "early stop\n",
      "best: 3 0.8793688635672292 0.6306451612903226\n"
     ]
    }
   ],
   "source": [
    "train_data,test_data=db.get_data(task=1,batch_size=8)\n",
    "age_model=AttentionNetwork(3)\n",
    "age_model.fit(train_data,test_data,'weibo_user_age.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dutir/chenxiaoyu/毕设最终代码/keras_extension/models.py:6: UserWarning: Update your `YModel` call to the Keras 2 API: `YModel(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  super(YModel, self).__init__(**kwargs)\n",
      "/home/dutir/chenxiaoyu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 110800200 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 train loss 1.854467990398407 acc 0.265     \n",
      "testing...best_acc\n",
      " 1 test 10:47:17 loss:1.772715, acc:0.283871\n",
      "-----\n",
      " 2 train loss 1.7965607452392578 acc 0.30625     \n",
      "testing...best_acc\n",
      " 2 test 10:47:25 loss:1.616714, acc:0.441129\n",
      "-----\n",
      " 3 train loss 1.6314248901605606 acc 0.4175     \n",
      "testing...best_acc\n",
      " 3 test 10:47:33 loss:1.446806, acc:0.504839\n",
      "-----\n",
      " 4 train loss 1.493607525229454 acc 0.49875     \n",
      "testing...best_acc\n",
      " 4 test 10:47:42 loss:1.323449, acc:0.565323\n",
      "-----\n",
      " 5 train loss 1.419806314110756 acc 0.52125     \n",
      "testing...best_acc\n",
      " 5 test 10:47:50 loss:1.238807, acc:0.593548\n",
      "-----\n",
      " 6 train loss 1.4332402783632279 acc 0.5225     \n",
      "testing...best_acc\n",
      " 6 test 10:47:58 loss:1.266591, acc:0.604032\n",
      "-----\n",
      " 7 train loss 1.3745384508371352 acc 0.5475     \n",
      "testing...best_acc\n",
      " 7 test 10:48:06 loss:1.335580, acc:0.610484\n",
      "-----\n",
      " 8 train loss 1.3000300869345665 acc 0.57375     \n",
      "testing...best_acc\n",
      " 8 test 10:48:14 loss:1.170720, acc:0.642742\n",
      "-----\n",
      " 9 train loss 1.3051726090908051 acc 0.57625     \n",
      " 9 test 10:48:20 loss:1.210248, acc:0.641129\n",
      "-----\n",
      " 10 train loss 1.2944900372624397 acc 0.605     \n",
      "testing...best_acc\n",
      " 10 test 10:48:28 loss:1.139641, acc:0.652419\n",
      "-----\n",
      " 11 train loss 1.2340363010764122 acc 0.5775     \n",
      " 11 test 10:48:33 loss:1.186997, acc:0.652419\n",
      "-----\n",
      " 12 train loss 1.1981518253684045 acc 0.61375     \n",
      " 12 test 10:48:39 loss:1.230377, acc:0.648387\n",
      "-----\n",
      " 13 train loss 1.2848465758562089 acc 0.59875     \n",
      "testing...best_acc\n",
      " 13 test 10:48:47 loss:1.111947, acc:0.658871\n",
      "-----\n",
      " 14 train loss 1.208642325103283 acc 0.62875     \n",
      "testing...best_acc\n",
      " 14 test 10:48:56 loss:1.101044, acc:0.665323\n",
      "-----\n",
      " 15 train loss 1.1817110173404217 acc 0.625     \n",
      "testing...best_acc\n",
      " 15 test 10:49:04 loss:1.113844, acc:0.686290\n",
      "-----\n",
      " 16 train loss 1.2038386419415474 acc 0.605     \n",
      " 16 test 10:49:10 loss:1.081477, acc:0.677419\n",
      "-----\n",
      " 17 train loss 1.1112528727948665 acc 0.65375     \n",
      " 17 test 10:49:15 loss:1.137571, acc:0.673387\n",
      "-----\n",
      " 18 train loss 1.1540542021393776 acc 0.64     \n",
      " 18 test 10:49:21 loss:1.089265, acc:0.679032\n",
      "-----\n",
      " 19 train loss 1.1826264598965646 acc 0.62125     \n",
      " 19 test 10:49:27 loss:1.141138, acc:0.675806\n",
      "-----\n",
      " 20 train loss 1.0830990719795226 acc 0.65625     \n",
      " 20 test 10:49:32 loss:1.195494, acc:0.667742\n",
      "-----\n",
      " 21 train loss 1.1488074196875095 acc 0.64875     \n",
      " 21 test 10:49:38 loss:1.171721, acc:0.674194\n",
      "-----\n",
      " 22 train loss 1.1594719830155373 acc 0.63     \n",
      " 22 test 10:49:44 loss:1.112163, acc:0.684677\n",
      "-----\n",
      " 23 train loss 1.0759767284989357 acc 0.64125     \n",
      "testing...best_acc\n",
      " 23 test 10:49:52 loss:1.140372, acc:0.688710\n",
      "-----\n",
      " 24 train loss 1.0844924945384264 acc 0.65125     \n",
      " 24 test 10:49:58 loss:1.178543, acc:0.671774\n",
      "-----\n",
      " 25 train loss 1.0919043278694154 acc 0.66     \n",
      " 25 test 10:50:03 loss:1.122538, acc:0.686290\n",
      "-----\n",
      " 26 train loss 1.1173133124411105 acc 0.645     \n",
      " 26 test 10:50:09 loss:1.122201, acc:0.682258\n",
      "-----\n",
      " 27 train loss 1.0522715564072131 acc 0.66125     \n",
      " 27 test 10:50:15 loss:1.225838, acc:0.685484\n",
      "-----\n",
      " 28 train loss 1.0754112461209298 acc 0.65625     \n",
      " 28 test 10:50:20 loss:1.166453, acc:0.674194\n",
      "-----\n",
      " 29 train loss 1.0998722504079341 acc 0.65125     \n",
      " 29 test 10:50:26 loss:1.160432, acc:0.687097\n",
      "-----\n",
      " 30 train loss 1.088670777976513 acc 0.6675     \n",
      " 30 test 10:50:31 loss:1.118022, acc:0.686290\n",
      "-----\n",
      " 31 train loss 1.0713230489194394 acc 0.65625     \n",
      "testing...best_acc\n",
      " 31 test 10:50:39 loss:1.116971, acc:0.691935\n",
      "-----\n",
      " 32 train loss 1.0496130791306495 acc 0.66375     \n",
      " 32 test 10:50:45 loss:1.131551, acc:0.684677\n",
      "-----\n",
      " 33 train loss 1.0518294101953507 acc 0.675     \n",
      " 33 test 10:50:51 loss:1.195585, acc:0.688710\n",
      "-----\n",
      " 34 train loss 1.0180328412353992 acc 0.66875     \n",
      " 34 test 10:50:57 loss:1.176892, acc:0.683871\n",
      "-----\n",
      " 35 train loss 0.9407225568592549 acc 0.71625     \n",
      " 35 test 10:51:02 loss:1.184459, acc:0.684677\n",
      "-----\n",
      " 36 train loss 1.0487092109024525 acc 0.645     \n",
      " 36 test 10:51:08 loss:1.145435, acc:0.679032\n",
      "-----\n",
      " 37 train loss 0.9679843750596047 acc 0.67     \n",
      " 37 test 10:51:14 loss:1.201213, acc:0.687903\n",
      "-----\n",
      " 38 train loss 0.991568271741271 acc 0.68875     \n",
      " 38 test 10:51:19 loss:1.160410, acc:0.687903\n",
      "-----\n",
      " 39 train loss 0.8533718229830265 acc 0.73125     \n",
      " 39 test 10:51:25 loss:1.168293, acc:0.680645\n",
      "-----\n",
      " 40 train loss 0.9282970790565014 acc 0.7075     \n",
      " 40 test 10:51:31 loss:1.191059, acc:0.681452\n",
      "-----\n",
      " 41 train loss 0.9079890079796314 acc 0.71625     \n",
      " 41 test 10:51:37 loss:1.172799, acc:0.680645\n",
      "-----\n",
      " 42 train loss 0.8289119093120099 acc 0.71625     \n",
      " 42 test 10:51:42 loss:1.177678, acc:0.674194\n",
      "-----\n",
      " 43 train loss 0.8520865125022828 acc 0.73375     \n",
      " 43 test 10:51:48 loss:1.215168, acc:0.674194\n",
      "-----\n",
      " 44 train loss 0.8302123798429966 acc 0.71625     \n",
      " 44 test 10:51:54 loss:1.208922, acc:0.670968\n",
      "-----\n",
      " 45 train loss 0.8357423004508019 acc 0.745     \n",
      " 45 test 10:52:00 loss:1.190153, acc:0.676613\n",
      "-----\n",
      " 46 train loss 0.7348291530460119 acc 0.78     \n",
      " 46 test 10:52:05 loss:1.205905, acc:0.672581\n",
      "-----\n",
      " 47 train loss 0.750020237462595 acc 0.78625     \n",
      " 47 test 10:52:11 loss:1.207426, acc:0.665323\n",
      "-----\n",
      " 48 train loss 0.7019337373226882 acc 0.79125     \n",
      " 48 test 10:52:16 loss:1.234692, acc:0.645968\n",
      "-----\n",
      " 49 train loss 0.707185383811593 acc 0.805     \n",
      " 49 test 10:52:22 loss:1.252567, acc:0.633871\n",
      "-----\n",
      " 50 train loss 0.6098405703157187 acc 0.80625     \n",
      " 50 test 10:52:28 loss:1.279617, acc:0.631452\n",
      "-----\n",
      "best: 30 1.1169712575693285 0.6919354838709677\n"
     ]
    }
   ],
   "source": [
    "train_data,test_data=db.get_data(task=2,batch_size=8)\n",
    "loca_model=AttentionNetwork(8)\n",
    "loca_model.fit(train_data,test_data,'weibo_user_location.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
